#!/usr/bin/env python

import sys, os, subprocess
import argparse
from argparse import RawTextHelpFormatter

###################################################
### 0. script description and parsing arguments ###
###################################################

synopsis1 = "\
  - creates OrthNet with homologous loci as nodes and 'CL_type' as edges,\n\
     using the '*.4OrthNet.input' file created by 'CL_finder_multi.py -n'.\n"
synopsis2 = "detailed description:\n\
 1. Input files and parameters\n\
  - expects './<Project>.4OrthNet.input' generated by 'CL_finder_multi.py -n',\n\
     i.e. a tab-delimited table of query, subject, and CL_type, with query and\n\
     subject loci IDs (quryID and sbjtID) formatted as 'spcsID|geneID'.\n\
  - '-d'|'--add_TD_edges': adds edges connecting tandem duplicated (TD) paralogs\n\
     ; requires './<Project>.list' including all spcsIDs, one per each line,\n\
     and 'TDfiles', output files of 'TD_finder.py' for all spcsIDs.\n\
  - '-t'|'--Path2TDfiles': path to 'TDfiles', required for '-d' option.\n\
  - '-T'|'--TDfile_nameFmt': expects TDfiles named as spcsID + TDfile_nameFmt\n\
 2. Clustering process and parameters\n\
  -  with '-d' option, adds TD edges to './<Project>.4OrthNet.input' and creates\n\
     a file named './<Project>.4OrthNet.TDadded.input'.\n\
  - creates the OrthNet with directional edges quryID -> sbjtID with CL_type as\n\
     the edge attribute; all connected egdes are clustered together; clusters\n\
     are given a numerical 'clstrID', sorted from the biggest to smallest.\n\
 3. Output files\n\
  - the following two files will be generated:\n\
     (1) <Project>.clstrd.nodes: clstrID, nodeID, occurance as query, and\n\
        occurance as subject, tab-delimited, one node per line.\n\
     (2) <Project>.clstrd.edges: clstrID, quryID, sbjtID, CL_type(q->s), and\n\
       CL_type(s->q), tab-delimited, one edge per line.\n\
  - '-o'|'--Path2Output' defines the output path (dafault='./Output'). \n\
  - '-s'|'--single_copies': add a column to both output files to identify\n\
     OrthNets containing all nodes from different species('1') or not('0'); if\n\
     '-d' is also specified, only non-TD nodes are considered.\n\n\
 by ohdongha@gmail.com ver0.2 20160822\n"
 
#version_history
#20180301 ver 0.2.1 # accept TD_finder.py output file formats as a string ('-T' option); if <Project> argument ends with '.list', just ignore it
#20160822 ver 0.2 # adding option to create edges for TD genes ('-t' option)
#20160812 ver 0.1 # modified to work with the updated 'CL_finder_multi.py'
#20160705 ver 0.0.1 # modifying output order for .nodes
#20160505 ver 0.0

parser = argparse.ArgumentParser(description = synopsis1, epilog = synopsis2, formatter_class = RawTextHelpFormatter)
 
## positional arguments and options:
parser.add_argument('Project', type=str, help="See below")
parser.add_argument('-d', '--add_TD_edges', action="store_true", default=False, help="add edges connecting TD nodes; see below")
parser.add_argument('-s', '--single_copies', action="store_true", default=False, help="see below")
parser.add_argument('-t', '--Path2TDfiles', dest="Path2TDfiles", type=str, default=".", help="PATH to 'TDfiles', required for -d; default='.'")
parser.add_argument('-T', '--TDfile_nameFmt', dest="TDfile_nameFmt", type=str, default=".", help="default='.gtfParsed.pc.TD.txt'; see below")
parser.add_argument('-o', '--Path2Output', dest="Path2Output", type=str, default="./Output", help="PATH for output files; default='./Output'") 
args = parser.parse_args()

# defining PATHs and create Output directory, if not already exisiting
path_TDfiles = args.Path2TDfiles
if path_TDfiles[-1] != "/": path_TDfiles = path_TDfiles + "/"
path_output = args.Path2Output
if path_output[-1] != "/": path_output = path_output + "/"

try: 
	os.makedirs(path_output)
except OSError:
	if not os.path.isdir(path_output): raise

## defining expected format of input and output file names.  modify as needed:
input_TDfiles_filename_format = path_TDfiles + "%s" + args.TDfile_nameFmt


#######################################################
### 1. adding TD edges, if '-d' option is specified ###
#######################################################

if args.add_TD_edges == True:

	### 2.1. reading the list file 
	try:
		fin_SpcsList = open(args.Project + '.list', 'r')
	except IOError:	
		fin_SpcsList = open(args.Project, 'r')		
	spcsID_list = []
	
	print "\nreading the list file:" + fin_SpcsList.name
	for line in fin_SpcsList:
		spcsID_list.append(line.strip())
		print line.strip()
	fin_SpcsList.close()
	print "Total %d species IDs detected." % (len(spcsID_list))
	
	### 2.2 extract TD genes from 'TDfiles'
	nodes_in_TD_dict = dict() # key = TDid, value = list of nodeIDs (as 'spcdID|geneID')
	nodeID_TDid_dict = dict() # key = nodeID ('spcsID|geneID'), value = TDid
	TDid = ""

	for i in range( len(spcsID_list) ):
		spcsID = spcsID_list[i]
		fin_TD = open( input_TDfiles_filename_format % spcsID, 'rU')
		header = True
		print "extracting TD nodes from %s" % spcsID
		num_TD_nodes = 0
		num_TDid = 0
		
		for line in fin_TD:
			if header:
				header = False
			else:
				tok = line.split('\t')
				TDid = tok[13].strip()
				if TDid != '-' and TDid not in nodes_in_TD_dict:
					num_TDid = num_TDid + 1
					num_TD_nodes = num_TD_nodes + 1
					nodes_in_TD_dict[TDid] = [ spcsID + '|' + tok[0] ]
					nodeID_TDid_dict[ spcsID + '|' + tok[0] ] = TDid # this is to be used for '-s'
				elif TDid != '-':
					num_TD_nodes = num_TD_nodes + 1
					nodes_in_TD_dict[TDid].append( spcsID + '|' + tok[0] )
					nodeID_TDid_dict[ spcsID + '|' + tok[0] ] = TDid # this is to be used for '-s'
		print "%d nodes in %d events have been collected," % (num_TD_nodes, num_TDid)

	### 2.3. adding TD_nodes to fin_4OrthNet
	subprocess.call("cp " + args.Project + '.4OrthNet.input ' + args.Project + '.4OrthNet.TDadded.input', shell=True)		
	fout_4OrthNet_fxd = open(args.Project + '.4OrthNet.TDadded.input', "a")
	
	for key in nodes_in_TD_dict:
		for i in range( len( nodes_in_TD_dict[key] ) - 1 ):
			fout_4OrthNet_fxd.write( nodes_in_TD_dict[key][i] + '\t' + nodes_in_TD_dict[key][i+1] + '\tTD\n')
	fout_4OrthNet_fxd.close()	

	
###############################################################################
### 2. reading <Project>.4OrthNet.input, counting effective nodes and edges ###
###############################################################################

if args.add_TD_edges == True:
	fin_4OrthNet = open(args.Project + '.4OrthNet.TDadded.input', "rU")
else:
	fin_4OrthNet = open(args.Project + '.4OrthNet.input', "rU")
fout_nodes = open(path_output + args.Project + ".clstrd.nodes", "w")
fout_edges = open(path_output + args.Project + ".clstrd.edges", "w")
fout_log = open(path_output + args.Project + ".clstrd.log", "w")		

# for all nodes, key = "nodeID" (i.e. "node1ID" or "node2ID"), value = occurance in <NAME.4OrthNet.input>: 
nodeID_count_as_query_dict = dict()
nodeID_count_as_subject_dict = dict()
# for all edges with <node1ID> alphnumerically smaller than <node2ID> , key = "node1ID___node2ID" (i.e. "edgeID"), value = CLtype :
edgeID_CLtype12_dict = dict()
edgeID_CLtype21_dict = dict()

node1ID = ""
node2ID = ""
edgeID = ""
CLtype = ""

print "reading %s" % fin_4OrthNet.name

for line in fin_4OrthNet:
	tok = line.split('\t')

	if len(tok) >= 3:
		node1ID = tok[0].strip()
		node2ID = tok[1].strip()
		CLtype = tok[2].strip()

		if CLtype == 'TD': # add nodes to the count dictionary, but do not increase the number
			nodeID_count_as_query_dict[node1ID] = nodeID_count_as_query_dict.get(node1ID, 0)
			nodeID_count_as_subject_dict[node2ID] = nodeID_count_as_subject_dict.get(node2ID, 0)		
			edgeID = node1ID + '___' + node2ID
			edgeID_CLtype12_dict[edgeID] = CLtype
			edgeID_CLtype21_dict[edgeID] = CLtype			
		else : # count nodeIDs as query or subject, only for non-TD edges
			nodeID_count_as_query_dict[node1ID] = nodeID_count_as_query_dict.get(node1ID, 0) + 1
			nodeID_count_as_subject_dict[node2ID] = nodeID_count_as_subject_dict.get(node2ID, 0) + 1		
			if node1ID < node2ID :
				edgeID = node1ID + '___' + node2ID
				edgeID_CLtype12_dict[edgeID] = CLtype
			elif node1ID > node2ID :
				edgeID = node2ID + '___' + node1ID
				edgeID_CLtype21_dict[edgeID] = CLtype
fin_4OrthNet.close()
			
for key in nodeID_count_as_query_dict:
	if key not in nodeID_count_as_subject_dict: nodeID_count_as_subject_dict[key] = 0
for key in nodeID_count_as_subject_dict:
	if key not in nodeID_count_as_query_dict: nodeID_count_as_query_dict[key] = 0
for key in edgeID_CLtype12_dict:
	if key not in edgeID_CLtype21_dict: edgeID_CLtype21_dict[key] = "-"
for key in edgeID_CLtype21_dict:
	if key not in edgeID_CLtype12_dict: edgeID_CLtype12_dict[key] = "-"

print "Total %d nodes and %d edges identified in %s." % ( len(nodeID_count_as_query_dict), len(edgeID_CLtype12_dict), fin_4OrthNet.name )
fin_4OrthNet.close()

	
###########################
### 3. clustering edges ###
###########################

print "Clustering begins."		
nodes_in_cluster_dict = dict() # key = "clusterID_unsorted", value = sets of nodeIDs 
nodes_in_cluster_dict[0] = set([])

last_clusterID = 0
cluster_found = 0
query_clusterID = 0
subject_clusterID = 0
clusterID_unsorted = 0
clusterID_2bMerged = 0 
number_of_processed_edges = 0
number_of_merged_clusters = 0

for edgeID in edgeID_CLtype12_dict:
	node1ID = edgeID.split('___')[0].strip()
	node2ID = edgeID.split('___')[1].strip()
	query_clusterID = 0
	subject_clusterID = 0
	cluster_found = 0
	number_of_processed_edges = number_of_processed_edges + 1
	
	# counter display
	if ( number_of_processed_edges % 10000 == 0):
		sys.stdout.write("\r   processed %d edges, %d cluster created and %d merged (i.e. effective cluster = %d)." \
			% (number_of_processed_edges, last_clusterID, number_of_merged_clusters, last_clusterID - number_of_merged_clusters) )
		sys.stdout.flush()
	
	# check whether the edge belong to existing clusters
	for clusterID in nodes_in_cluster_dict:
		if cluster_found == 0 or cluster_found == 2: 
			if node1ID in nodes_in_cluster_dict[clusterID]:
				cluster_found = cluster_found + 1
				query_clusterID = clusterID
		if cluster_found == 0 or cluster_found == 1: 
			if node2ID in nodes_in_cluster_dict[clusterID]:
				cluster_found = cluster_found + 2
				subject_clusterID = clusterID
		if cluster_found == 3: 
			break

	# if either nodes is in any existing cluster, create a new cluster, 				
	# if both nodes are in the same existing cluster, add both to the cluster, and
	# if nodes are in different existing clusters, merge clusters to the one with the lower clusterID, empty the one with the higher clusterID, and add both to the merged cluster :
	if cluster_found == 0:
		clusterID_unsorted = last_clusterID
		last_clusterID = last_clusterID + 1
		nodes_in_cluster_dict[last_clusterID] = set([])
	elif cluster_found == 1:
		clusterID_unsorted = query_clusterID
	elif cluster_found == 2:
		clusterID_unsorted = subject_clusterID
	elif cluster_found == 3:
		if query_clusterID == subject_clusterID:
			clusterID_unsorted = query_clusterID
		else:
			clusterID_unsorted = min( query_clusterID, subject_clusterID )
			clusterID_2bMerged = max( query_clusterID, subject_clusterID )
			nodes_in_cluster_dict[clusterID_unsorted] = nodes_in_cluster_dict[clusterID_unsorted].union(nodes_in_cluster_dict[clusterID_2bMerged])
			del nodes_in_cluster_dict[clusterID_2bMerged]
			number_of_merged_clusters = number_of_merged_clusters + 1
	nodes_in_cluster_dict[clusterID_unsorted].add(node1ID)
	nodes_in_cluster_dict[clusterID_unsorted].add(node2ID)
print "\nfinished processing total %d edges in %s, %d clusters identified." % (number_of_processed_edges, sys.argv[1], last_clusterID - number_of_merged_clusters)


###############################################################
### 4. assign clusterID_unsorted to all nodeIDs and edgeIDs ###
###############################################################

print "assigning clusterID to all nodeIDs and edgeIDs"
# for all nodes, "nodeID" (i.e. "node1ID" or "node2ID"), value = "clusterID_unsorted": 
nodeID_clusterID_dict = dict()
# for all edges, key = "node1ID___node2ID" (i.e. "edgeID"), value = "clusterID_unsorted":
edgeID_clusterID_dict = dict()

for clusterID_unsorted in nodes_in_cluster_dict:
	for nodeID in nodes_in_cluster_dict[clusterID_unsorted]:
		nodeID_clusterID_dict[nodeID] = clusterID_unsorted

for edgeID in edgeID_CLtype12_dict:
	node1ID = edgeID.split('___')[0].strip()
	node2ID = edgeID.split('___')[1].strip()
	if nodeID_clusterID_dict[node1ID] == nodeID_clusterID_dict[node2ID]:
		edgeID_clusterID_dict[edgeID] = nodeID_clusterID_dict[node1ID]
	else:
		print "clustering error in edgeID: %s (node1ID in cluster %d, while node2ID in cluster %d. " % (edgeID, nodeID_clusterID_dict[node1ID], nodeID_clusterID_dict[node2ID])

		
##########################################################
#### 5. count number of nodes in each cluster and sort ###
##########################################################

clusterID_sorted = 0

# for all clusters, key = "clusterID_unsorted", value = number of nodes in the cluster: 
cluster_size_dict = dict()
# for all clusters, with "clusterID_unsorted" as the key, and "clusterID_sorted" as the value: 
cluster_sorted_order_dict = dict()

print "sorting clusters."

for clusterID in nodes_in_cluster_dict:
	cluster_size_dict[clusterID] = len(nodes_in_cluster_dict[clusterID])
	
for key in sorted(cluster_size_dict, key=cluster_size_dict.get, reverse=True):
	cluster_sorted_order_dict[key] = clusterID_sorted
	clusterID_sorted = clusterID_sorted + 1
	fout_log.write("clusterID_unsorted = %d, _size = %d, _sorted = %d\n" % (key, cluster_size_dict[key], clusterID_sorted))
print "finished sorting %d clusters." % clusterID_sorted

fout_log.close()

###############################################################
### 6. create .nodes and .edges files; process '-s' option  ###
###############################################################

# for all clusters, key = "clusterID_unsorted", value = 'singleCopies' cluster (1) or not (0)
cluster_singleCopies_dict = dict()	
num_singleCopies_clusters = 0
num_clusters_with_TD_nodes = 0 

spcsID = ""

if args.single_copies == False:
	fout_nodes.write("nodeID\tclusterID\tcount_as_query\tcount_as_subject\tcount_as_both\n")
	fout_edges.write("clusterID\tnode1ID\tnode2ID\tCLtype[1->2]\tCLtype[2->1]\n")		
else :
	fout_nodes.write("nodeID\tclusterID\tcount_as_query\tcount_as_subject\tcount_as_both\tsingle_copies_only?\n")
	fout_edges.write("clusterID\tnode1ID\tnode2ID\tCLtype[1->2]\tCLtype[2->1]\tsingle_copies_only?\n")		
	print "counting OrthNets with single copy gene nodes only."
	for clusterID_unsorted in nodes_in_cluster_dict:
		cluster_singleCopies_dict[clusterID_unsorted] = "\t1"
		nodes_reduced_in_cluster = set([])
		species_occured_in_cluster = set([])

		# if '-d', reduce TD nodes to single TDid in each clstrs		
		if args.add_TD_edges == True:
			for nodeID in nodes_in_cluster_dict[clusterID_unsorted]:
				if nodeID in nodeID_TDid_dict:
					nodes_reduced_in_cluster.add( nodeID_TDid_dict[nodeID] ) # TDid starts with 'spcsID|', too.
				else:
					nodes_reduced_in_cluster.add(nodeID)
		else:
			nodes_reduced_in_cluster = nodes_in_cluster_dict[clusterID_unsorted]

		if len(nodes_reduced_in_cluster) < len(nodes_in_cluster_dict[clusterID_unsorted]):
			num_clusters_with_TD_nodes = num_clusters_with_TD_nodes + 1
		
		# decide whether the cluster includes 'single copy' nodes only
		for nodeID in nodes_reduced_in_cluster: 
			spcsID = nodeID.split('|')[0].strip()
			if spcsID in species_occured_in_cluster:
				cluster_singleCopies_dict[clusterID_unsorted] = "\t0"
				break
			else:
				species_occured_in_cluster.add(spcsID)				
#		print ','.join(nodes_reduced_in_cluster) + cluster_singleCopies_dict[clusterID_unsorted] # this was for debugging, ...
		
	for key in cluster_singleCopies_dict:
		if cluster_singleCopies_dict[key] == "\t1":
			num_singleCopies_clusters = num_singleCopies_clusters + 1
			
	if args.add_TD_edges == True:
		print "out of %d clusters, %d contain TD nodes" % (len(nodes_in_cluster_dict), num_clusters_with_TD_nodes )
	print "out of %d OrthNets, %d contain only single copy gene nodes." % (last_clusterID - number_of_merged_clusters, num_singleCopies_clusters)


##############################	
### 7. create output files ###
##############################	

# variables to create output files
clusterID_unsorted = 0
node1ID = ""
node2ID = ""

print "creating output files."

for nodeID, clusterID_unsorted in nodeID_clusterID_dict.items(): 
	fout_nodes.write( nodeID + '\t' + str(cluster_sorted_order_dict[clusterID_unsorted]) + '\t' + str(nodeID_count_as_query_dict[nodeID]) + '\t' + str(nodeID_count_as_subject_dict[nodeID]) \
		+ '\t' + str(nodeID_count_as_query_dict[nodeID] + nodeID_count_as_subject_dict[nodeID]) + cluster_singleCopies_dict.get(clusterID_unsorted, "") + '\n')
			
for edgeID, clusterID_unsorted in edgeID_clusterID_dict.items():
	node1ID = edgeID.split('___')[0].strip()
	node2ID = edgeID.split('___')[1].strip()
	fout_edges.write( str(cluster_sorted_order_dict[clusterID_unsorted]) + '\t' + node1ID + '\t' + node2ID + '\t' + edgeID_CLtype12_dict[edgeID] + '\t' + edgeID_CLtype21_dict[edgeID]  \
		+ cluster_singleCopies_dict.get(clusterID_unsorted, "") + '\n')
fout_nodes.close()
fout_edges.close()

# sort report files
command1 = "awk 'NR == 1; NR > 1 {print $0 | \"sort -k2,2n -k5,5nr\"}' " + path_output + args.Project + ".clstrd.nodes > __clstrhd_temp__; mv __clstrhd_temp__ " + path_output + args.Project + ".clstrd.nodes"
command2 = "awk 'NR == 1; NR > 1 {print $0 | \"sort -k1,1n -k2,2nr\"}' " + path_output + args.Project + ".clstrd.edges > __clstrhd_temp__; mv __clstrhd_temp__ " + path_output + args.Project + ".clstrd.edges"
subprocess.call(command1, shell=True)
subprocess.call(command2, shell=True)

print("done")